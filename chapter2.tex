%===================================== CHAP 2 =================================

\chapter{Literature Review/Background}

%Web and experience, enric pliaza, ralph bergmann, CBR stuff cooking recepies

%Mining stories from web
\section{Related work}
Discuss other articles that touch on the same subject

\subsection{Text data mining}
This projects aims at extracting specific sections from articles on Wikipedia and then find relations among them. This puts this project in between two fields. On one side there is information retrieval(IR), which is mainly is about helping users finding documents of their needs\cite{irbook}. On the other side is text data mining(TDM), where the goal is to discover useful information from textual data. Methods for accomplishing this could for instance be finding patterns across data sets, or finding relevant information among a collection of mostly irrelevant data\cite{untanglingTDM}.

To be able to discover relevant data from the Wikipedia articles, Wikimedia's markup language has to be interpreted. Section \ref{wikimedia} explains further about Wikimedia and the syntax of its markup, this section will focus on the structure we can derive from the markup. This structure mainly consists of section headers and the content of an section below, although there is also other information that can be extracted. To find relations between articles we would also need to look at the semantics from this information, were the most important is lists, categories and references, amongst others. YAWN\footnote{Yet Another Wikipedia Annotation project}, is a project that created an XML version of Wikipedia with focus on semantic information\cite{yawn}. The XML corpus produced by YAWN is general for the whole Wikipedia and the entire articles. This project only focuses on the articles relevant for educational purposes and possessing examples to explain its content. Although YAWN overreaches our purpose, key concepts and techniques for exploiting the Wiki markup, can be derived and used for classifying examples in this project. The most interesting for this project is how they find semantic annotations for Wikipedia pages. For that, they use a combination of exploiting information from categories assigned to articles and deriving information from the structure of an article.

\subsection{Wikimedia} \label{wikimedia}
Wikimedia is an organization responsible for many different Wiki sites, among them the well known Wikipedia. More about them and about the Wiki syntax

As already mentioned to extract information from Wikipedia based on the given XML-dump, the syntax of the markup used for writing articles has to be interpreted. 

\subsection{SMILA} \label{smila}

\begin{figure}[h]
\caption{An overall overview of the SMILA architecture}
\includegraphics[width=\textwidth]{SMILA_Architecture}
\end{figure}

%%fix sånn abbrevation for SMILA

%%når jeg snakker om mine erfaringer med smila gjør jeg det i implementation.

%%Skriv om dette til å passe til sin nye kontekst
In the early stages of the project, an existing project called SMILA was explored. SMILA is a system with its first release in 2010, used to search and access unstructured information. SMILA crawls the web to extract information and then stores the information in an index. It has a REST API to control the system and for searching the index. The SMILA architecture is also based on the pipeline architecture containing the following processes; jobs, crawling, storage, indexing and querying. Since 2010, 6 new versions has been released adding more features to SMILA. With SMILA being very complex, it gains asynchronicity as its biggest benefit from the pipeline architecture. The SMILA pipeline also allows custom made pipelets to be inserted into the pipeline. A pipelet is a sub process inside a pipeline. By creating pipelets, the behaviour of SMILA could be tailored into extracting the relevant information from Wikipedia.

The releases for SMILA has been dwindling the last three years with only 1 release in 2015. If you add that to the fact that all the different features of SMILA makes it very complex, the usability and stability suffers. This was experienced during testing of the program during this project.
Nevertheless the utility that the SMILA system offers could be taken advantage of in this project. Either by utilizing SMILA itself, or look at how SMILA retrieves data from the web, processes it and produces a data set as a result.


\section{Tools / Methodology}

\subsection{ElasticSearch} \label{elasticsearch}
%%JSON abbrevation
ElasticSearch is a tool used in this project for indexing the examples. ElasticSearch is built on top of Apache Lucene(https://lucene.apache.org), which is a information retrieval library, written in Java. Internally in ElasticSearch, data is stored as structured JSON\footnote{JSON - JavaScript Object Notation} documents. The API for communicating with ElasticSearch is a RESTful\footnote{REST - representational state transfer} API using JSON over HTTP. The API can be used for configuring ElasticSearch, building the index and querying it. 
%%Ta med i erfaring seksjon om dette at siden alt dette bruker json og webserveren bruker javascript via node, er alt veldig enkelt og konsistent.

ElasticSearch is built for scalability. This means that it can handle the dataset and interactions growing. This is because it acts as a cluster of many nodes. If the system needs to scale, new nodes can easily be added, and ElasticSearch will distribute resources to the newly added ndoes. However this projects does not need or take advantage of this scaling, and will only be using one node.

When searching in ElasticSearch, there is mainly two ways of doing this. The first one is by using \textit{filter}. The \textit{filter} is utilizing \textit{term} to decide whether a document should be returned or not. Searching with \textit{term} is very similar to how one would use SQL. %Trenger denne forklaring?
Searches can for instance consist of text strings, numbers, ranges or dates, and ElasticSearch will return everything that matches. It also allows for boolean operators and nesting of these. Using a \textit{filter} is very quick and should be used if the relevance of the documents is not important. If relevance score is important then the second option, \textit{query}, should be chosen. If a \textit{query} is combined with a \textit{term}, ElasticSearch is looking for the exact value in its index. A score is then returned based on the documents TF/IDF relevance to the term. See section \label{tfidf} for an explanation of this algorithm.%Burde det referes på denne måten?
If a \textit{match} is used instead, an analysis will be performed, creating a list of terms from the query, and then executing low-level queries for each of the terms. The results are combined to produce the final relevance score. These two methods can also be combined or extended with other methods to customize the search further.


\subsection{NPM and Node.js}

\section{Techniques}

\subsection{Pipeline} \label{pipeline}
Describe a software pipeline. Advantages and stuff.\\
How should i use references for this?

\subsection{TF/IDF} \label{tf/idf}
TF/IDF is an algorithm which calculates how important an word is to a document based on the document itself and the collection it is part of. Based on this it is possible to decide how likely it is for the document to be relevant. TF/IDF is used as the standard similarity algorithm in ElasticSearch, see section \ref{elasticsearch}.

TF/IDF is can be divided into two parts, \textit{term frequency} and \textit{inverse document frequency}. \textit{Term frequency} is how often a term appears in a document. The more instances the document has of the word, the higher is the chance of the document being relevant. \texit{Inverse document frequency} looks at how often a term appears in the whole collection of documents. The more often a term appears, the less relevant is the term. This means the common terms will have less weight than rare ones, when calculating the likelihood of the documents relevance. 

\cleardoublepage